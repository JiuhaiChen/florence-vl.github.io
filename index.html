<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>



<!-- Inline Styles for Black Background -->
<style>
  /* Black background and white text for the first section */
  .hero {
      background-color: black;
      color: white;
  }

  /* Link styling */
  a {
      color: #1E90FF; /* Bright blue for links */
  }

  a:hover {
      color: #FFD700; /* Gold on hover */
  }
</style>
</head>
<body>


  
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="color: white;">Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=eJP77eoAAAAJ&hl=en">Jiuhai Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://jwyang.github.io/">Jianwei Yang</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=Yv-H6F4AAAAJ&hl=en">Haiping Wu</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=K40nbiQAAAAJ&hl=en">Dianqi Li</a><sup></sup>,
            </span>
            <span class="author-block">
              <a href="https://www.microsoft.com/en-us/research/people/jfgao/">Jianfeng Gao</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://tianyizhou.github.io/">Tianyi Zhou</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="https://leoxiaobin.github.io/">Bin Xiao</a><sup>2</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Maryland,</span>
            <span class="author-block"><sup>2</sup>Microsoft Research</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2412.04424"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/JiuhaiChen/Florence-VL"  
                   class="external-link button is-normal is-rounded is-dark">  
                  <span>Code</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/JiuhaiChen/Florence-VL"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Data</span>
                  </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/jiuhai/Florence-VL-8B"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Demo (8B)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/spaces/jiuhai/Florence-VL-3B"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Demo (3B)</span>
                  </a>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/jiuhai/florence-vl-8b-sft"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Checkpoint (8B)</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://huggingface.co/jiuhai/florence-vl-3b-sft"
                   class="external-link button is-normal is-rounded is-dark">
                  <span>Checkpoint (3B)</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->





<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            1. We present Florence-VL, a new family of multimodal large language models (MLLMs) with enriched visual representations produced by Florence-2, a generative vision foundation model. Unlike the widely used CLIP-style vision transformer trained by contrastive learning, Florence-2 can capture different levels and aspects of visual features, which are more versatile to be adapted to diverse downstream tasks. 
          </p>
          <p>
            2. We propose a novel feature-fusion architecture and an innovative training recipe that effectively integrates Florence-2's visual features into pretrained LLMs, such as Phi 3.5 and LLama 3. In particular, we propose "<em>depth-breath fusion</em>" (DBFusion) to fuse the visual features extracted from different depths and under multiple prompts.
            Our model training is composed of end-to-end pretraining of the whole model followed by finetuning of the projection layer and the LLM, on a carefully designed recipe of diverse open-source datasets that include high-quality image captions and instruction-tuning pairs. 
          </p>
            3. Our quantitative analysis and visualization of Florence-VL's visual features show its advantages over popular vision encoders on vision-language alignment, where the enriched depth and breath play important roles. Florence-VL achieves significant improvements over existing state-of-the-art MLLMs across various multi-modal and vision-centric benchmarks covering general VQA, perception, hallucination, OCR, Chart, knowledge-intensive understanding, etc. To facilitate future research, our models and the complete training recipe are open-sourced.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

  </div>
</section>





<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">

      <!-- Visual Effects. -->
      <div class="column central-content">
        <div class="content">
            <h2 class="title is-3">LLaVA v.s. Florence-VL</h2>
            <div class="content" style="text-align: left;">
            <p style="margin-bottom: 80px;">
              Comparison of LLaVA-style MLLMs with our Florence-VL. LLaVA-style models use CLIP, pretrained with contrastive learning, to generate a <strong>single high-level image feature</strong>. In contrast, Florence-VL leverages Florence-2, pretrained with generative modeling across various vision tasks such as image captioning, OCR, and grounding. This enables Florence-2 to flexibly extract <strong>multiple task-specific image features</strong> using Florence-2 as the image encoder.
            </p>

            <img id="dollyzoom-image" src="./static/images/teaser.png" alt="Dolly Zoom Effect" />
        </div>


        <div class="column central-content">
          <div class="content">
              <h2 class="title is-3">Florence-VL</h2>
              <div class="content" style="text-align: left;">
                <p style="margin-bottom: 80px;">
                  An overview of Florence-VL, which extracts visual features of different depths and breaths from Florence-2, combines them using DBFusion, and project the fused features to an LLM's input space. 
                </p>
                <img id="dollyzoom-image" src="./static/images/method.png" alt="Dolly Zoom Effect" />

                <div>
                  <p>
                    <strong>Breadth.</strong> We focus on three distinct tasks that contribute to image understanding, resulting in three different image embeddings:
                  </p>
                  <ul style="margin-top: 10px;">
                    <li>
                      <strong>Detailed Image Caption:</strong> Describe what is shown in the image with a paragraph.
                    </li>
                    <li>
                      <strong>OCR:</strong> Provide the text shown in the image.
                    </li>
                    <li>
                      <strong>Dense Region Caption:</strong> Locate the objects in the image, with their descriptions. 
                    </li>
                  </ul>
                </div>
                <p style="margin-top: 20px;"></p>
                  <strong>Depth.</strong> We also integrate lower-level features from DaViT combined with higher-level features.
                </p>
                <p style="margin-top: 20px;"></p>
                <strong>DB-fusion.</strong> We concatenate features along the channel dimension, showing better performance and training efficiency than other fusion strategy.
              </p>
                
          </div>


        <div class="column central-content">
          <div class="content">
              <h2 class="title is-3">Visualization</h2>
              <div class="content" style="text-align: left;">
                  Visualization of the first three PCA components: we apply PCA to image features generated from Detailed Caption, OCR, and Grounding prompts, excluding the background by setting a threshold on the first PCA component.                  </p>
                    <li>
                      The image features derived from the Detailed Caption prompt (second column) capture the general context of the image.
                    </li>
                    <li>
                      Those from the OCR prompt (third column) focus primarily on text information.
                    </li>
                    <li>
                      Those from the Grounding prompt (fourth column) highlight spatial relationships between objects.
                    </li>
                  </ul>
                  <p style="margin-top: 20px;"></p>
               Additionally, we visualize the final layer features from OpenAI CLIP (ViT-L/14@336) in the last column, showing that CLIP features often miss certain region-level details, such as text information in many cases. 
                </p>
                <p style="margin-top: 80px;">
                <img id="dollyzoom-image" src="./static/images/florence_feature.png" alt="Dolly Zoom Effect" />
          </div>

    
          <div class="column central-content">
            <div class="content">
                <h2 class="title is-3">Training Recipe</h2>
                <div class="content" style="text-align: left;">
                <p>
                  <strong>17M Detailed Image Caption.</strong> In order to build a state-of-the-art MLLM, we use images from CC12M, Redcaps, and Commonpool during the pretraining stage, with detailed captions sourced from <a href="https://huggingface.co/datasets/tomg-group-umd/pixelprose" target="_blank">PixelProse</a> and <a href="https://huggingface.co/datasets/Lin-Chen/ShareGPT4V" target="_blank">ShareGPT4V</a> . 
                </p>
                  <strong>10M High Quality Instruction Data.</strong> For the instruction tuning stage, we also curate our high quality instruction tuning datasets, sourcing from Cambrian-7M, Vision Flan, ShareGPT4V, along with additional data from Docmatix to improve chart and diagram comprehension. 
            </div>






          <div class="column central-content">
            <div class="content">
                <h2 class="title is-3">State of the Art MLLM Performance</h2>
                <div class="content" style="text-align: left;">
                <p>
                  <img id="dollyzoom-image" src="./static/images/results.png" alt="Dolly Zoom Effect" />
                  <p style="margin-top: 80px;">
            </div>

    
    </div>
    
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <!-- <div class="column">
        <h2 class="title is-3">Matting</h2>
        <div class="columns is-centered">
          <div class="column content">
            <p>
              As a byproduct of our method, we can also solve the matting problem by ignoring
              samples that fall outside of a bounding box during rendering.
            </p>
            <video id="matting-video" controls playsinline height="100%">
              <source src="./static/videos/matting.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div> -->
    <!--/ Matting. -->

    <!-- Animation. -->
    <!-- <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
        <!-- <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        <!-- <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p>
        </div>
        <div class="content has-text-centered">
          <video id="replay-video"
                 controls
                 muted
                 preload
                 playsinline
                 width="75%">
            <source src="./static/videos/replay.mp4"
                    type="video/mp4">
          </video>
        </div> -->
        <!--/ Re-rendering. -->

      </div>
    </div>
    <!--/ Animation. -->


  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2024florence,
  title={Florence-VL: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion},
  author={Chen, Jiuhai and Yang, Jianwei and Wu, Haiping and Li, Dianqi and Gao, Jianfeng and Zhou, Tianyi and Xiao, Bin},
  journal={arXiv preprint arXiv:2412.04424},
  year={2024}
}</code></pre>
  </div>
</section>

